{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB10.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oK7kZzy7r6VT",
        "tXRXS4AEr9KK",
        "B7TeuIhY7Hqw",
        "0SKtK5Q57KB7",
        "V33jISFw_Dkc",
        "nRvMjwr0C-fh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCCtuiZA9s4O"
      },
      "source": [
        "import tensorflow        as tf\n",
        "import numpy             as np\n",
        "import pandas            as pd\n",
        "import os\n",
        "import seaborn           as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_theme()\n",
        "\n",
        "from sklearn.model_selection              import train_test_split\n",
        "from sklearn.metrics                      import accuracy_score\n",
        "from sklearn.preprocessing                import MinMaxScaler\n",
        "from tensorflow.keras                     import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from google.colab                         import drive, output\n",
        "\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "output.clear()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaq4bWZZCjVF"
      },
      "source": [
        "BASE_PATH         = \"/content/drive/MyDrive/Datasets/large-multiomic\"\n",
        "\n",
        "METH_FILENAME     = f\"{BASE_PATH}/meth.txt\"\n",
        "CLUSTERS_FILENAME = f\"{BASE_PATH}/clusters.txt\"\n",
        "MRNA_FILENAME     = f\"{BASE_PATH}/mRNA.txt\"\n",
        "PROTEINS_FILENAME = f\"{BASE_PATH}/prot.txt\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VsCAgtYCmt2",
        "outputId": "372c349e-d73b-43d3-acc7-0531985228fe"
      },
      "source": [
        "_, cluster_data, mRNA_data, _ = load_data(METH_FILENAME, CLUSTERS_FILENAME, MRNA_FILENAME, PROTEINS_FILENAME)\n",
        "\n",
        "'''\n",
        "Merge mRNA data with cluster data\n",
        "'''\n",
        "data = mRNA_data.merge(cluster_data, left_index = True, right_index = True)\n",
        "data.cluster = data.cluster - 1 # [1,2,3,4,5] => [0,1,2,3,4]\n",
        "data = normalize_dataframe(data)\n",
        "\n",
        "'''\n",
        "Labels inspection\n",
        "'''\n",
        "for i in range(5):\n",
        "  print(f\"{len(data[data.cluster == i])}\\t rows belonging to cluster {i}\")"
      ],
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1250\t rows belonging to cluster 0\n",
            "1000\t rows belonging to cluster 1\n",
            "1000\t rows belonging to cluster 2\n",
            "1000\t rows belonging to cluster 3\n",
            "750\t rows belonging to cluster 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "3-lVO5UqEFU3",
        "outputId": "f2b39b93-4ab1-4117-bde6-80e355d9e7c8"
      },
      "source": [
        "data"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.333966</td>\n",
              "      <td>0.701087</td>\n",
              "      <td>0.640290</td>\n",
              "      <td>0.554411</td>\n",
              "      <td>0.263794</td>\n",
              "      <td>0.520564</td>\n",
              "      <td>0.633027</td>\n",
              "      <td>0.715708</td>\n",
              "      <td>0.593011</td>\n",
              "      <td>0.770871</td>\n",
              "      <td>0.601214</td>\n",
              "      <td>0.661777</td>\n",
              "      <td>0.743597</td>\n",
              "      <td>0.373324</td>\n",
              "      <td>0.339426</td>\n",
              "      <td>0.518860</td>\n",
              "      <td>0.655779</td>\n",
              "      <td>0.508005</td>\n",
              "      <td>0.724453</td>\n",
              "      <td>0.568271</td>\n",
              "      <td>0.311726</td>\n",
              "      <td>0.668638</td>\n",
              "      <td>0.393210</td>\n",
              "      <td>0.462265</td>\n",
              "      <td>0.717649</td>\n",
              "      <td>0.898011</td>\n",
              "      <td>0.517031</td>\n",
              "      <td>0.652542</td>\n",
              "      <td>0.687342</td>\n",
              "      <td>0.625478</td>\n",
              "      <td>0.409188</td>\n",
              "      <td>0.907177</td>\n",
              "      <td>0.787653</td>\n",
              "      <td>0.666129</td>\n",
              "      <td>0.373977</td>\n",
              "      <td>0.388621</td>\n",
              "      <td>0.304410</td>\n",
              "      <td>0.401956</td>\n",
              "      <td>0.214215</td>\n",
              "      <td>0.251225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.245063</td>\n",
              "      <td>0.398635</td>\n",
              "      <td>0.609120</td>\n",
              "      <td>0.487678</td>\n",
              "      <td>0.241784</td>\n",
              "      <td>0.350986</td>\n",
              "      <td>0.424556</td>\n",
              "      <td>0.339125</td>\n",
              "      <td>0.146922</td>\n",
              "      <td>0.653079</td>\n",
              "      <td>0.627667</td>\n",
              "      <td>0.820508</td>\n",
              "      <td>0.747603</td>\n",
              "      <td>0.723057</td>\n",
              "      <td>0.699102</td>\n",
              "      <td>0.469287</td>\n",
              "      <td>0.562113</td>\n",
              "      <td>0.515307</td>\n",
              "      <td>0.707903</td>\n",
              "      <td>0.675619</td>\n",
              "      <td>0.699560</td>\n",
              "      <td>0.684142</td>\n",
              "      <td>0.294362</td>\n",
              "      <td>0.612973</td>\n",
              "      <td>0.610320</td>\n",
              "      <td>0.270315</td>\n",
              "      <td>0.409818</td>\n",
              "      <td>0.349892</td>\n",
              "      <td>0.314287</td>\n",
              "      <td>0.608358</td>\n",
              "      <td>0.709199</td>\n",
              "      <td>0.611894</td>\n",
              "      <td>0.763897</td>\n",
              "      <td>0.239128</td>\n",
              "      <td>0.795876</td>\n",
              "      <td>0.573995</td>\n",
              "      <td>0.498396</td>\n",
              "      <td>0.621601</td>\n",
              "      <td>0.423564</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.660550</td>\n",
              "      <td>0.232089</td>\n",
              "      <td>0.163024</td>\n",
              "      <td>0.368027</td>\n",
              "      <td>0.512814</td>\n",
              "      <td>0.413925</td>\n",
              "      <td>0.619938</td>\n",
              "      <td>0.501474</td>\n",
              "      <td>0.388359</td>\n",
              "      <td>0.685889</td>\n",
              "      <td>0.282104</td>\n",
              "      <td>0.217180</td>\n",
              "      <td>0.653216</td>\n",
              "      <td>0.328424</td>\n",
              "      <td>0.263171</td>\n",
              "      <td>0.457918</td>\n",
              "      <td>0.259363</td>\n",
              "      <td>0.641285</td>\n",
              "      <td>0.748272</td>\n",
              "      <td>0.389873</td>\n",
              "      <td>0.787723</td>\n",
              "      <td>0.482979</td>\n",
              "      <td>0.578719</td>\n",
              "      <td>0.456308</td>\n",
              "      <td>0.625706</td>\n",
              "      <td>0.501337</td>\n",
              "      <td>0.343199</td>\n",
              "      <td>0.583325</td>\n",
              "      <td>0.593592</td>\n",
              "      <td>0.584766</td>\n",
              "      <td>0.320873</td>\n",
              "      <td>0.607802</td>\n",
              "      <td>0.697916</td>\n",
              "      <td>0.450335</td>\n",
              "      <td>0.407549</td>\n",
              "      <td>0.753733</td>\n",
              "      <td>0.530084</td>\n",
              "      <td>0.850533</td>\n",
              "      <td>0.260282</td>\n",
              "      <td>0.333913</td>\n",
              "      <td>...</td>\n",
              "      <td>0.309921</td>\n",
              "      <td>0.421868</td>\n",
              "      <td>0.163854</td>\n",
              "      <td>0.348857</td>\n",
              "      <td>0.676055</td>\n",
              "      <td>0.498178</td>\n",
              "      <td>0.809410</td>\n",
              "      <td>0.272555</td>\n",
              "      <td>0.225680</td>\n",
              "      <td>0.548581</td>\n",
              "      <td>0.428224</td>\n",
              "      <td>0.170571</td>\n",
              "      <td>0.351538</td>\n",
              "      <td>0.269103</td>\n",
              "      <td>0.434495</td>\n",
              "      <td>0.594730</td>\n",
              "      <td>0.656786</td>\n",
              "      <td>0.710843</td>\n",
              "      <td>0.697030</td>\n",
              "      <td>0.264503</td>\n",
              "      <td>0.685218</td>\n",
              "      <td>0.429756</td>\n",
              "      <td>0.487203</td>\n",
              "      <td>0.387162</td>\n",
              "      <td>0.660174</td>\n",
              "      <td>0.734660</td>\n",
              "      <td>0.239916</td>\n",
              "      <td>0.284330</td>\n",
              "      <td>0.258149</td>\n",
              "      <td>0.675536</td>\n",
              "      <td>0.503108</td>\n",
              "      <td>0.695346</td>\n",
              "      <td>0.325266</td>\n",
              "      <td>0.274333</td>\n",
              "      <td>0.782965</td>\n",
              "      <td>0.653487</td>\n",
              "      <td>0.474091</td>\n",
              "      <td>0.488940</td>\n",
              "      <td>0.210421</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.190614</td>\n",
              "      <td>0.704563</td>\n",
              "      <td>0.518615</td>\n",
              "      <td>0.426099</td>\n",
              "      <td>0.717759</td>\n",
              "      <td>0.600128</td>\n",
              "      <td>0.201792</td>\n",
              "      <td>0.582330</td>\n",
              "      <td>0.733987</td>\n",
              "      <td>0.741479</td>\n",
              "      <td>0.558475</td>\n",
              "      <td>0.546625</td>\n",
              "      <td>0.168709</td>\n",
              "      <td>0.784263</td>\n",
              "      <td>0.644294</td>\n",
              "      <td>0.611409</td>\n",
              "      <td>0.280085</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.684365</td>\n",
              "      <td>0.715968</td>\n",
              "      <td>0.398558</td>\n",
              "      <td>0.434014</td>\n",
              "      <td>0.711927</td>\n",
              "      <td>0.537393</td>\n",
              "      <td>0.617665</td>\n",
              "      <td>0.386991</td>\n",
              "      <td>0.756917</td>\n",
              "      <td>0.529259</td>\n",
              "      <td>0.539363</td>\n",
              "      <td>0.775787</td>\n",
              "      <td>0.897086</td>\n",
              "      <td>0.374206</td>\n",
              "      <td>0.641379</td>\n",
              "      <td>0.410906</td>\n",
              "      <td>0.572751</td>\n",
              "      <td>0.850527</td>\n",
              "      <td>0.570599</td>\n",
              "      <td>0.696218</td>\n",
              "      <td>0.644826</td>\n",
              "      <td>0.826833</td>\n",
              "      <td>...</td>\n",
              "      <td>0.302387</td>\n",
              "      <td>0.364944</td>\n",
              "      <td>0.758862</td>\n",
              "      <td>0.726495</td>\n",
              "      <td>0.763099</td>\n",
              "      <td>0.473008</td>\n",
              "      <td>0.700586</td>\n",
              "      <td>0.358483</td>\n",
              "      <td>0.686634</td>\n",
              "      <td>0.298796</td>\n",
              "      <td>0.715593</td>\n",
              "      <td>0.280091</td>\n",
              "      <td>0.736409</td>\n",
              "      <td>0.220029</td>\n",
              "      <td>0.456736</td>\n",
              "      <td>0.796928</td>\n",
              "      <td>0.850050</td>\n",
              "      <td>0.244542</td>\n",
              "      <td>0.757874</td>\n",
              "      <td>0.705213</td>\n",
              "      <td>0.732018</td>\n",
              "      <td>0.476759</td>\n",
              "      <td>0.458280</td>\n",
              "      <td>0.493961</td>\n",
              "      <td>0.298475</td>\n",
              "      <td>0.568045</td>\n",
              "      <td>0.340523</td>\n",
              "      <td>0.289717</td>\n",
              "      <td>0.674140</td>\n",
              "      <td>0.483979</td>\n",
              "      <td>0.538242</td>\n",
              "      <td>0.600650</td>\n",
              "      <td>0.320671</td>\n",
              "      <td>0.720311</td>\n",
              "      <td>0.296580</td>\n",
              "      <td>0.280545</td>\n",
              "      <td>0.633265</td>\n",
              "      <td>0.225341</td>\n",
              "      <td>0.608042</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.789362</td>\n",
              "      <td>0.308383</td>\n",
              "      <td>0.176440</td>\n",
              "      <td>0.452512</td>\n",
              "      <td>0.607311</td>\n",
              "      <td>0.395980</td>\n",
              "      <td>0.583258</td>\n",
              "      <td>0.650997</td>\n",
              "      <td>0.142194</td>\n",
              "      <td>0.689592</td>\n",
              "      <td>0.353632</td>\n",
              "      <td>0.148877</td>\n",
              "      <td>0.757226</td>\n",
              "      <td>0.603736</td>\n",
              "      <td>0.347251</td>\n",
              "      <td>0.369944</td>\n",
              "      <td>0.266391</td>\n",
              "      <td>0.661066</td>\n",
              "      <td>0.688848</td>\n",
              "      <td>0.465430</td>\n",
              "      <td>0.680332</td>\n",
              "      <td>0.641976</td>\n",
              "      <td>0.735272</td>\n",
              "      <td>0.488929</td>\n",
              "      <td>0.644639</td>\n",
              "      <td>0.634820</td>\n",
              "      <td>0.362399</td>\n",
              "      <td>0.362373</td>\n",
              "      <td>0.726775</td>\n",
              "      <td>0.965942</td>\n",
              "      <td>0.345215</td>\n",
              "      <td>0.695377</td>\n",
              "      <td>0.632772</td>\n",
              "      <td>0.441513</td>\n",
              "      <td>0.558312</td>\n",
              "      <td>0.630791</td>\n",
              "      <td>0.604454</td>\n",
              "      <td>0.608835</td>\n",
              "      <td>0.153000</td>\n",
              "      <td>0.275375</td>\n",
              "      <td>...</td>\n",
              "      <td>0.288994</td>\n",
              "      <td>0.647770</td>\n",
              "      <td>0.204964</td>\n",
              "      <td>0.335042</td>\n",
              "      <td>0.801496</td>\n",
              "      <td>0.307066</td>\n",
              "      <td>0.728420</td>\n",
              "      <td>0.296815</td>\n",
              "      <td>0.312305</td>\n",
              "      <td>0.750742</td>\n",
              "      <td>0.512253</td>\n",
              "      <td>0.254139</td>\n",
              "      <td>0.334234</td>\n",
              "      <td>0.369990</td>\n",
              "      <td>0.630193</td>\n",
              "      <td>0.439304</td>\n",
              "      <td>0.606431</td>\n",
              "      <td>0.832753</td>\n",
              "      <td>0.647688</td>\n",
              "      <td>0.269049</td>\n",
              "      <td>0.765771</td>\n",
              "      <td>0.578282</td>\n",
              "      <td>0.570934</td>\n",
              "      <td>0.429984</td>\n",
              "      <td>0.522692</td>\n",
              "      <td>0.767029</td>\n",
              "      <td>0.277008</td>\n",
              "      <td>0.372408</td>\n",
              "      <td>0.343562</td>\n",
              "      <td>0.567464</td>\n",
              "      <td>0.505327</td>\n",
              "      <td>0.484641</td>\n",
              "      <td>0.513227</td>\n",
              "      <td>0.231792</td>\n",
              "      <td>0.583012</td>\n",
              "      <td>0.269958</td>\n",
              "      <td>0.317902</td>\n",
              "      <td>0.497963</td>\n",
              "      <td>0.212104</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.635404</td>\n",
              "      <td>0.272905</td>\n",
              "      <td>0.329159</td>\n",
              "      <td>0.538752</td>\n",
              "      <td>0.380842</td>\n",
              "      <td>0.421012</td>\n",
              "      <td>0.670941</td>\n",
              "      <td>0.650685</td>\n",
              "      <td>0.353216</td>\n",
              "      <td>0.630704</td>\n",
              "      <td>0.407100</td>\n",
              "      <td>0.325641</td>\n",
              "      <td>0.675232</td>\n",
              "      <td>0.320258</td>\n",
              "      <td>0.333267</td>\n",
              "      <td>0.430529</td>\n",
              "      <td>0.176926</td>\n",
              "      <td>0.743377</td>\n",
              "      <td>0.703511</td>\n",
              "      <td>0.311281</td>\n",
              "      <td>0.687354</td>\n",
              "      <td>0.477146</td>\n",
              "      <td>0.665785</td>\n",
              "      <td>0.482401</td>\n",
              "      <td>0.455719</td>\n",
              "      <td>0.761369</td>\n",
              "      <td>0.400186</td>\n",
              "      <td>0.631210</td>\n",
              "      <td>0.681327</td>\n",
              "      <td>0.553609</td>\n",
              "      <td>0.308028</td>\n",
              "      <td>0.885196</td>\n",
              "      <td>0.747574</td>\n",
              "      <td>0.443305</td>\n",
              "      <td>0.265710</td>\n",
              "      <td>0.650713</td>\n",
              "      <td>0.157942</td>\n",
              "      <td>0.754769</td>\n",
              "      <td>0.330849</td>\n",
              "      <td>0.334131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.259240</td>\n",
              "      <td>0.339164</td>\n",
              "      <td>0.324148</td>\n",
              "      <td>0.460206</td>\n",
              "      <td>0.595811</td>\n",
              "      <td>0.291365</td>\n",
              "      <td>0.860479</td>\n",
              "      <td>0.381568</td>\n",
              "      <td>0.208329</td>\n",
              "      <td>0.796723</td>\n",
              "      <td>0.544736</td>\n",
              "      <td>0.241357</td>\n",
              "      <td>0.178378</td>\n",
              "      <td>0.209853</td>\n",
              "      <td>0.543413</td>\n",
              "      <td>0.706722</td>\n",
              "      <td>0.498724</td>\n",
              "      <td>0.857119</td>\n",
              "      <td>0.592594</td>\n",
              "      <td>0.243212</td>\n",
              "      <td>0.677280</td>\n",
              "      <td>0.570564</td>\n",
              "      <td>0.674788</td>\n",
              "      <td>0.473231</td>\n",
              "      <td>0.723364</td>\n",
              "      <td>0.693811</td>\n",
              "      <td>0.414517</td>\n",
              "      <td>0.301009</td>\n",
              "      <td>0.402043</td>\n",
              "      <td>0.587530</td>\n",
              "      <td>0.599467</td>\n",
              "      <td>0.667786</td>\n",
              "      <td>0.258448</td>\n",
              "      <td>0.297671</td>\n",
              "      <td>0.827771</td>\n",
              "      <td>0.802755</td>\n",
              "      <td>0.430466</td>\n",
              "      <td>0.574227</td>\n",
              "      <td>0.254884</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>0.642136</td>\n",
              "      <td>0.271989</td>\n",
              "      <td>0.273637</td>\n",
              "      <td>0.375342</td>\n",
              "      <td>0.598100</td>\n",
              "      <td>0.241495</td>\n",
              "      <td>0.679818</td>\n",
              "      <td>0.601123</td>\n",
              "      <td>0.311407</td>\n",
              "      <td>0.675418</td>\n",
              "      <td>0.409016</td>\n",
              "      <td>0.254352</td>\n",
              "      <td>0.697204</td>\n",
              "      <td>0.457720</td>\n",
              "      <td>0.372839</td>\n",
              "      <td>0.281959</td>\n",
              "      <td>0.250189</td>\n",
              "      <td>0.661793</td>\n",
              "      <td>0.528902</td>\n",
              "      <td>0.381626</td>\n",
              "      <td>0.571764</td>\n",
              "      <td>0.522348</td>\n",
              "      <td>0.832736</td>\n",
              "      <td>0.350321</td>\n",
              "      <td>0.643236</td>\n",
              "      <td>0.820587</td>\n",
              "      <td>0.255438</td>\n",
              "      <td>0.591586</td>\n",
              "      <td>0.493656</td>\n",
              "      <td>0.702589</td>\n",
              "      <td>0.369134</td>\n",
              "      <td>0.833440</td>\n",
              "      <td>0.566325</td>\n",
              "      <td>0.552296</td>\n",
              "      <td>0.342851</td>\n",
              "      <td>0.677779</td>\n",
              "      <td>0.530747</td>\n",
              "      <td>0.656894</td>\n",
              "      <td>0.422274</td>\n",
              "      <td>0.267127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.221844</td>\n",
              "      <td>0.632413</td>\n",
              "      <td>0.376810</td>\n",
              "      <td>0.452963</td>\n",
              "      <td>0.731398</td>\n",
              "      <td>0.407402</td>\n",
              "      <td>0.784123</td>\n",
              "      <td>0.188991</td>\n",
              "      <td>0.302529</td>\n",
              "      <td>0.586637</td>\n",
              "      <td>0.230494</td>\n",
              "      <td>0.266237</td>\n",
              "      <td>0.227938</td>\n",
              "      <td>0.334551</td>\n",
              "      <td>0.389187</td>\n",
              "      <td>0.740805</td>\n",
              "      <td>0.580658</td>\n",
              "      <td>0.642693</td>\n",
              "      <td>0.600398</td>\n",
              "      <td>0.356312</td>\n",
              "      <td>0.670384</td>\n",
              "      <td>0.759840</td>\n",
              "      <td>0.431899</td>\n",
              "      <td>0.353102</td>\n",
              "      <td>0.874148</td>\n",
              "      <td>0.719669</td>\n",
              "      <td>0.375133</td>\n",
              "      <td>0.327729</td>\n",
              "      <td>0.392948</td>\n",
              "      <td>0.610719</td>\n",
              "      <td>0.288182</td>\n",
              "      <td>0.589573</td>\n",
              "      <td>0.221255</td>\n",
              "      <td>0.345689</td>\n",
              "      <td>0.854060</td>\n",
              "      <td>0.689329</td>\n",
              "      <td>0.542164</td>\n",
              "      <td>0.737161</td>\n",
              "      <td>0.181298</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>0.263072</td>\n",
              "      <td>0.545620</td>\n",
              "      <td>0.161361</td>\n",
              "      <td>0.427608</td>\n",
              "      <td>0.276648</td>\n",
              "      <td>0.430425</td>\n",
              "      <td>0.279677</td>\n",
              "      <td>0.733719</td>\n",
              "      <td>0.317651</td>\n",
              "      <td>0.670179</td>\n",
              "      <td>0.212445</td>\n",
              "      <td>0.511756</td>\n",
              "      <td>0.178789</td>\n",
              "      <td>0.176209</td>\n",
              "      <td>0.213573</td>\n",
              "      <td>0.399269</td>\n",
              "      <td>0.268093</td>\n",
              "      <td>0.386414</td>\n",
              "      <td>0.546413</td>\n",
              "      <td>0.651599</td>\n",
              "      <td>0.323546</td>\n",
              "      <td>0.307678</td>\n",
              "      <td>0.788818</td>\n",
              "      <td>0.346346</td>\n",
              "      <td>0.600592</td>\n",
              "      <td>0.463078</td>\n",
              "      <td>0.451779</td>\n",
              "      <td>0.518486</td>\n",
              "      <td>0.391073</td>\n",
              "      <td>0.626288</td>\n",
              "      <td>0.375665</td>\n",
              "      <td>0.752294</td>\n",
              "      <td>0.426978</td>\n",
              "      <td>0.198136</td>\n",
              "      <td>0.250615</td>\n",
              "      <td>0.391747</td>\n",
              "      <td>0.534295</td>\n",
              "      <td>0.689358</td>\n",
              "      <td>0.202234</td>\n",
              "      <td>0.538983</td>\n",
              "      <td>...</td>\n",
              "      <td>0.238082</td>\n",
              "      <td>0.626214</td>\n",
              "      <td>0.705256</td>\n",
              "      <td>0.379525</td>\n",
              "      <td>0.688908</td>\n",
              "      <td>0.496242</td>\n",
              "      <td>0.676207</td>\n",
              "      <td>0.695153</td>\n",
              "      <td>0.735486</td>\n",
              "      <td>0.397035</td>\n",
              "      <td>0.092694</td>\n",
              "      <td>0.873339</td>\n",
              "      <td>0.720482</td>\n",
              "      <td>0.055403</td>\n",
              "      <td>0.328102</td>\n",
              "      <td>0.281920</td>\n",
              "      <td>0.381066</td>\n",
              "      <td>0.140557</td>\n",
              "      <td>0.642452</td>\n",
              "      <td>0.121319</td>\n",
              "      <td>0.594859</td>\n",
              "      <td>0.667587</td>\n",
              "      <td>0.524943</td>\n",
              "      <td>0.214761</td>\n",
              "      <td>0.372008</td>\n",
              "      <td>0.261387</td>\n",
              "      <td>0.295977</td>\n",
              "      <td>0.639478</td>\n",
              "      <td>0.331495</td>\n",
              "      <td>0.633379</td>\n",
              "      <td>0.275033</td>\n",
              "      <td>0.707050</td>\n",
              "      <td>0.419015</td>\n",
              "      <td>0.837394</td>\n",
              "      <td>0.619780</td>\n",
              "      <td>0.534983</td>\n",
              "      <td>0.534206</td>\n",
              "      <td>0.243155</td>\n",
              "      <td>0.386483</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>0.654683</td>\n",
              "      <td>0.251318</td>\n",
              "      <td>0.327646</td>\n",
              "      <td>0.681759</td>\n",
              "      <td>0.672203</td>\n",
              "      <td>0.563503</td>\n",
              "      <td>0.506486</td>\n",
              "      <td>0.800945</td>\n",
              "      <td>0.276786</td>\n",
              "      <td>0.663997</td>\n",
              "      <td>0.429274</td>\n",
              "      <td>0.191611</td>\n",
              "      <td>0.754633</td>\n",
              "      <td>0.414880</td>\n",
              "      <td>0.264620</td>\n",
              "      <td>0.299069</td>\n",
              "      <td>0.246355</td>\n",
              "      <td>0.627880</td>\n",
              "      <td>0.843094</td>\n",
              "      <td>0.417124</td>\n",
              "      <td>0.856726</td>\n",
              "      <td>0.581357</td>\n",
              "      <td>0.667450</td>\n",
              "      <td>0.453170</td>\n",
              "      <td>0.573932</td>\n",
              "      <td>0.648225</td>\n",
              "      <td>0.343129</td>\n",
              "      <td>0.635908</td>\n",
              "      <td>0.544029</td>\n",
              "      <td>0.707460</td>\n",
              "      <td>0.301338</td>\n",
              "      <td>0.593567</td>\n",
              "      <td>0.714713</td>\n",
              "      <td>0.387773</td>\n",
              "      <td>0.316592</td>\n",
              "      <td>0.778661</td>\n",
              "      <td>0.413472</td>\n",
              "      <td>0.559303</td>\n",
              "      <td>0.200151</td>\n",
              "      <td>0.293313</td>\n",
              "      <td>...</td>\n",
              "      <td>0.167537</td>\n",
              "      <td>0.763064</td>\n",
              "      <td>0.286133</td>\n",
              "      <td>0.377887</td>\n",
              "      <td>0.604954</td>\n",
              "      <td>0.448508</td>\n",
              "      <td>0.541692</td>\n",
              "      <td>0.489965</td>\n",
              "      <td>0.259868</td>\n",
              "      <td>0.585593</td>\n",
              "      <td>0.489020</td>\n",
              "      <td>0.183658</td>\n",
              "      <td>0.209703</td>\n",
              "      <td>0.357216</td>\n",
              "      <td>0.392397</td>\n",
              "      <td>0.775180</td>\n",
              "      <td>0.669596</td>\n",
              "      <td>0.557581</td>\n",
              "      <td>0.673805</td>\n",
              "      <td>0.118760</td>\n",
              "      <td>0.701286</td>\n",
              "      <td>0.505118</td>\n",
              "      <td>0.508676</td>\n",
              "      <td>0.293054</td>\n",
              "      <td>0.694282</td>\n",
              "      <td>0.739174</td>\n",
              "      <td>0.173528</td>\n",
              "      <td>0.286639</td>\n",
              "      <td>0.212093</td>\n",
              "      <td>0.621528</td>\n",
              "      <td>0.361853</td>\n",
              "      <td>0.843894</td>\n",
              "      <td>0.362311</td>\n",
              "      <td>0.298905</td>\n",
              "      <td>0.711558</td>\n",
              "      <td>0.513569</td>\n",
              "      <td>0.355794</td>\n",
              "      <td>0.244687</td>\n",
              "      <td>0.304906</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>0.670192</td>\n",
              "      <td>0.319741</td>\n",
              "      <td>0.388904</td>\n",
              "      <td>0.592139</td>\n",
              "      <td>0.625152</td>\n",
              "      <td>0.730461</td>\n",
              "      <td>0.597119</td>\n",
              "      <td>0.715657</td>\n",
              "      <td>0.457240</td>\n",
              "      <td>0.726984</td>\n",
              "      <td>0.342960</td>\n",
              "      <td>0.340299</td>\n",
              "      <td>0.836398</td>\n",
              "      <td>0.527013</td>\n",
              "      <td>0.378340</td>\n",
              "      <td>0.309581</td>\n",
              "      <td>0.218063</td>\n",
              "      <td>0.590710</td>\n",
              "      <td>0.747469</td>\n",
              "      <td>0.339471</td>\n",
              "      <td>0.489026</td>\n",
              "      <td>0.594553</td>\n",
              "      <td>0.634369</td>\n",
              "      <td>0.323498</td>\n",
              "      <td>0.791897</td>\n",
              "      <td>0.610073</td>\n",
              "      <td>0.509570</td>\n",
              "      <td>0.353426</td>\n",
              "      <td>0.729543</td>\n",
              "      <td>0.495475</td>\n",
              "      <td>0.355656</td>\n",
              "      <td>0.650546</td>\n",
              "      <td>0.660012</td>\n",
              "      <td>0.487387</td>\n",
              "      <td>0.260175</td>\n",
              "      <td>0.590537</td>\n",
              "      <td>0.433543</td>\n",
              "      <td>0.790966</td>\n",
              "      <td>0.282058</td>\n",
              "      <td>0.319143</td>\n",
              "      <td>...</td>\n",
              "      <td>0.359154</td>\n",
              "      <td>0.427293</td>\n",
              "      <td>0.099801</td>\n",
              "      <td>0.449931</td>\n",
              "      <td>0.680648</td>\n",
              "      <td>0.425826</td>\n",
              "      <td>0.777799</td>\n",
              "      <td>0.287087</td>\n",
              "      <td>0.442037</td>\n",
              "      <td>0.567368</td>\n",
              "      <td>0.430673</td>\n",
              "      <td>0.242388</td>\n",
              "      <td>0.322103</td>\n",
              "      <td>0.415408</td>\n",
              "      <td>0.091448</td>\n",
              "      <td>0.530335</td>\n",
              "      <td>0.765032</td>\n",
              "      <td>0.761239</td>\n",
              "      <td>0.719480</td>\n",
              "      <td>0.409514</td>\n",
              "      <td>0.765939</td>\n",
              "      <td>0.640103</td>\n",
              "      <td>0.632189</td>\n",
              "      <td>0.321170</td>\n",
              "      <td>0.719929</td>\n",
              "      <td>0.753787</td>\n",
              "      <td>0.364318</td>\n",
              "      <td>0.445497</td>\n",
              "      <td>0.401465</td>\n",
              "      <td>0.645494</td>\n",
              "      <td>0.604857</td>\n",
              "      <td>0.569131</td>\n",
              "      <td>0.348231</td>\n",
              "      <td>0.175685</td>\n",
              "      <td>0.801973</td>\n",
              "      <td>0.742977</td>\n",
              "      <td>0.275309</td>\n",
              "      <td>0.430322</td>\n",
              "      <td>0.302410</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>0.297271</td>\n",
              "      <td>0.864792</td>\n",
              "      <td>0.466271</td>\n",
              "      <td>0.249193</td>\n",
              "      <td>0.803760</td>\n",
              "      <td>0.183815</td>\n",
              "      <td>0.160514</td>\n",
              "      <td>0.597613</td>\n",
              "      <td>0.634640</td>\n",
              "      <td>0.743842</td>\n",
              "      <td>0.647437</td>\n",
              "      <td>0.460522</td>\n",
              "      <td>0.217319</td>\n",
              "      <td>0.405139</td>\n",
              "      <td>0.725656</td>\n",
              "      <td>0.736144</td>\n",
              "      <td>0.327895</td>\n",
              "      <td>0.809004</td>\n",
              "      <td>0.571866</td>\n",
              "      <td>0.695210</td>\n",
              "      <td>0.413445</td>\n",
              "      <td>0.272817</td>\n",
              "      <td>0.713209</td>\n",
              "      <td>0.497322</td>\n",
              "      <td>0.718765</td>\n",
              "      <td>0.370393</td>\n",
              "      <td>0.411338</td>\n",
              "      <td>0.450242</td>\n",
              "      <td>0.484154</td>\n",
              "      <td>0.789421</td>\n",
              "      <td>0.636393</td>\n",
              "      <td>0.382482</td>\n",
              "      <td>0.712875</td>\n",
              "      <td>0.420759</td>\n",
              "      <td>0.815124</td>\n",
              "      <td>0.699051</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.595006</td>\n",
              "      <td>0.532860</td>\n",
              "      <td>0.554112</td>\n",
              "      <td>...</td>\n",
              "      <td>0.224899</td>\n",
              "      <td>0.430980</td>\n",
              "      <td>0.558019</td>\n",
              "      <td>0.760263</td>\n",
              "      <td>0.609885</td>\n",
              "      <td>0.690190</td>\n",
              "      <td>0.696247</td>\n",
              "      <td>0.264126</td>\n",
              "      <td>0.730057</td>\n",
              "      <td>0.388983</td>\n",
              "      <td>0.765711</td>\n",
              "      <td>0.175696</td>\n",
              "      <td>0.642583</td>\n",
              "      <td>0.380499</td>\n",
              "      <td>0.532052</td>\n",
              "      <td>0.752787</td>\n",
              "      <td>0.792399</td>\n",
              "      <td>0.416570</td>\n",
              "      <td>0.760974</td>\n",
              "      <td>0.587165</td>\n",
              "      <td>0.665601</td>\n",
              "      <td>0.582211</td>\n",
              "      <td>0.360345</td>\n",
              "      <td>0.779761</td>\n",
              "      <td>0.366860</td>\n",
              "      <td>0.793077</td>\n",
              "      <td>0.276401</td>\n",
              "      <td>0.141321</td>\n",
              "      <td>0.758816</td>\n",
              "      <td>0.688724</td>\n",
              "      <td>0.458469</td>\n",
              "      <td>0.807685</td>\n",
              "      <td>0.223638</td>\n",
              "      <td>0.725051</td>\n",
              "      <td>0.342923</td>\n",
              "      <td>0.506995</td>\n",
              "      <td>0.523223</td>\n",
              "      <td>0.515878</td>\n",
              "      <td>0.784222</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows Ã— 132 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2  ...       129       130  cluster\n",
              "0     0.333966  0.701087  0.640290  ...  0.621601  0.423564        4\n",
              "1     0.660550  0.232089  0.163024  ...  0.488940  0.210421        2\n",
              "2     0.190614  0.704563  0.518615  ...  0.225341  0.608042        3\n",
              "3     0.789362  0.308383  0.176440  ...  0.497963  0.212104        2\n",
              "4     0.635404  0.272905  0.329159  ...  0.574227  0.254884        2\n",
              "...        ...       ...       ...  ...       ...       ...      ...\n",
              "4995  0.642136  0.271989  0.273637  ...  0.737161  0.181298        2\n",
              "4996  0.263072  0.545620  0.161361  ...  0.243155  0.386483        0\n",
              "4997  0.654683  0.251318  0.327646  ...  0.244687  0.304906        2\n",
              "4998  0.670192  0.319741  0.388904  ...  0.430322  0.302410        2\n",
              "4999  0.297271  0.864792  0.466271  ...  0.515878  0.784222        3\n",
              "\n",
              "[5000 rows x 132 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 344
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "489w_ueS3eA8"
      },
      "source": [
        "### Basic Neural Network for multi-class classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK7kZzy7r6VT"
      },
      "source": [
        "#### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJiOmLxG4Z8a",
        "outputId": "d85cec38-62bc-45b5-b3a2-d1640131e03b"
      },
      "source": [
        "'''\n",
        "Divide features from labels\n",
        "'''\n",
        "X = data.drop(\"cluster\", axis = 1)\n",
        "y = data.cluster \n",
        "\n",
        "'''\n",
        "Divide dataset into train and test sets\n",
        "'''\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
        "\n",
        "'''\n",
        "Merge features and labels into train and test sets\n",
        "'''\n",
        "train = X_train.merge(y_train, left_index = True, right_index = True).astype(np.float32)\n",
        "test  = X_test .merge(y_test,  left_index = True, right_index = True).astype(np.float32)\n",
        "\n",
        "'''\n",
        "Verify that the split process went well :)\n",
        "'''\n",
        "print(\"# Train dataset #\")\n",
        "for i in range(5):\n",
        "  print(f\"{len(train[train.cluster == i])}\\t rows belonging to cluster {i}\")\n",
        "print()\n",
        "print(f\"{len(train)} train examples\")\n",
        "\n",
        "print() \n",
        "\n",
        "print(\"# Test dataset #\")\n",
        "for i in range(5):\n",
        "  print(f\"{len(test[test.cluster == i])}\\t rows belonging to cluster {i}\")\n",
        "print()\n",
        "print(f\"{len(test)} test examples\")"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Train dataset #\n",
            "1000\t rows belonging to cluster 0\n",
            "800\t rows belonging to cluster 1\n",
            "800\t rows belonging to cluster 2\n",
            "800\t rows belonging to cluster 3\n",
            "600\t rows belonging to cluster 4\n",
            "\n",
            "4000 train examples\n",
            "\n",
            "# Test dataset #\n",
            "250\t rows belonging to cluster 0\n",
            "200\t rows belonging to cluster 1\n",
            "200\t rows belonging to cluster 2\n",
            "200\t rows belonging to cluster 3\n",
            "150\t rows belonging to cluster 4\n",
            "\n",
            "1000 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVD3AvRE4dWj"
      },
      "source": [
        "'''\n",
        "One hot encoding\n",
        "'''\n",
        "y_train = tf.keras.utils.to_categorical(train.cluster)\n",
        "y_test  = tf.keras.utils.to_categorical(test.cluster)\n",
        "\n",
        "'''\n",
        "Drop labels from train and test sets\n",
        "'''\n",
        "train.drop(\"cluster\", inplace = True, axis = 1)\n",
        "test .drop(\"cluster\", inplace = True, axis = 1)\n",
        "\n",
        "\n",
        "'''\n",
        "Build TensorFlow train and test Datasets\n",
        "'''\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = df_to_ds(train, y_train,                 batch_size = batch_size)\n",
        "test_ds  = df_to_ds(test,  y_test, shuffle = False, batch_size = batch_size)"
      ],
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRXS4AEr9KK"
      },
      "source": [
        "#### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9dbeVzmq9R8",
        "outputId": "d0232155-c3a3-4942-f688-5d8357ef98e7"
      },
      "source": [
        "class NN:\n",
        "\n",
        "  def __init__(self, features, classes):\n",
        "\n",
        "    nn_input = tf.keras.layers.Input(shape = (features))\n",
        "    x = tf.keras.layers.Dropout(0.5)(nn_input)\n",
        "    x = tf.keras.layers.Dense(32)(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(classes, activation = \"sigmoid\")(x)\n",
        "\n",
        "    self.model = tf.keras.models.Model(nn_input, x)\n",
        "    self.model.summary()\n",
        "\n",
        "nn = NN(features = 131, classes = 5).model\n",
        "\n",
        "nn.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\n",
        "\n",
        "nn.fit(train_ds, epochs = 3)"
      ],
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_135 (InputLayer)       [(None, 131)]             0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 131)               0         \n",
            "_________________________________________________________________\n",
            "dense_518 (Dense)            (None, 32)                4224      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_519 (Dense)            (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 4,389\n",
            "Trainable params: 4,389\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "125/125 [==============================] - 1s 1ms/step - loss: 1.6641 - acc: 0.3892\n",
            "Epoch 2/3\n",
            "125/125 [==============================] - 0s 1ms/step - loss: 0.6357 - acc: 0.7846\n",
            "Epoch 3/3\n",
            "125/125 [==============================] - 0s 1ms/step - loss: 0.4017 - acc: 0.8681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f656da5d828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g7cGkrjr_Gu"
      },
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI-jdOHE40H3",
        "outputId": "d1ddf921-6813-4e92-c864-6a12e16274d9"
      },
      "source": [
        "_ = nn.evaluate(test_ds)"
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 949us/step - loss: 0.0371 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDYdqwpCArNm"
      },
      "source": [
        "### Assignment 1: Implement a GAN for sample generation\n",
        "\n",
        "Download dataset_5000samples.rar from the Teaching Portal. \n",
        "\n",
        "After decompressing the file, you will have: mRNA.txt, meth.txt, prot.txt which contain respectively transcriptome, genome and proteome of 5000 samples divided in 5 classes. \n",
        "\n",
        "In clusters.txt you can find the label number for each sample. For this assignment you need only the mRNA.txt file.\n",
        "\n",
        "Implement a GAN for each class in order to generate new samples starting from mRNA.txt file. <br>\n",
        "Implement an upper sampling technique using SMOTE.<br>\n",
        "Which are the differences between sample generation using GANs and SMOTE upper sampling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQlfr5JqsFCM"
      },
      "source": [
        "#### Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7TeuIhY7Hqw"
      },
      "source": [
        "#### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEGjg1i_3dPJ"
      },
      "source": [
        "class Discriminator:\n",
        "\n",
        "  def __init__(self, features, verbose = True):\n",
        "    '''\n",
        "    The discriminator should ...\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "        discr_input = tf.keras.layers.Input(shape = (features))\n",
        "        x = tf.keras.layers.Dropout(0.5)(discr_input)\n",
        "        x = tf.keras.layers.Dense(32)(x)\n",
        "        x = tf.keras.layers.Dropout(0.5)(x)\n",
        "        x = tf.keras.layers.Dense(classes, activation = \"sigmoid\")(x)\n",
        "\n",
        "        self.model = tf.keras.models.Model(nn_input, x)\n",
        "    '''\n",
        "\n",
        "    discr_input = tf.keras.layers.Input(shape = (features))\n",
        "    x = tf.keras.layers.Dense(512)(discr_input)\n",
        "    x = tf.keras.layers.Dense(256)(x)\n",
        "    x = tf.keras.layers.Dense(128)(x)\n",
        "    x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    self.model = tf.keras.models.Model(discr_input, x, name = \"Discriminator\")\n",
        "\n",
        "    if verbose == True:\n",
        "      print(self.model.summary(), end = \"\\n\\n\")\n",
        "\n",
        "  def get_loss(self, real_predictions, fake_predictions):\n",
        "\n",
        "    real_predictions = tf.sigmoid(real_predictions)\n",
        "    fake_predictions = tf.sigmoid(fake_predictions)\n",
        "\n",
        "    real_loss = tf.losses.binary_crossentropy(tf.ones_like(real_predictions),  real_predictions)\n",
        "    fake_loss = tf.losses.binary_crossentropy(tf.zeros_like(fake_predictions), fake_predictions)\n",
        "\n",
        "    return real_loss + fake_loss"
      ],
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SKtK5Q57KB7"
      },
      "source": [
        "#### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe67XRsY7BSS"
      },
      "source": [
        "class Generator():\n",
        "\n",
        "  def __init__(self, noise, features, verbose = True):\n",
        "    '''\n",
        "    The generator should ...\n",
        "    '''\n",
        "\n",
        "    gen_input = tf.keras.layers.Input(shape = (noise))\n",
        "    x = tf.keras.layers.Dense(128)(gen_input)\n",
        "    x = tf.keras.layers.Dense(256)(x)\n",
        "    x = tf.keras.layers.Dense(512, activation = \"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(features)(x)\n",
        "\n",
        "    self.model = tf.keras.models.Model(gen_input, x, name = \"Generator\")\n",
        "    \n",
        "    if verbose == True:\n",
        "      print(self.model.summary(), end = \"\\n\\n\")\n",
        "\n",
        "  def get_loss(self, fake_predictions):\n",
        "\n",
        "    fake_predictions = tf.sigmoid(fake_predictions)\n",
        "    \n",
        "    fake_loss = tf.losses.binary_crossentropy(tf.ones_like(fake_predictions), fake_predictions)\n",
        "\n",
        "    return fake_loss"
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V33jISFw_Dkc"
      },
      "source": [
        "#### TGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juu6WHzv_DMS"
      },
      "source": [
        "class TGAN(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, ds_features, input_shape_generator, verbose = True):\n",
        "\n",
        "    super(TGAN, self).__init__()\n",
        "\n",
        "    self.discriminator = Discriminator(features = ds_features, verbose = verbose)\n",
        "    self.generator     = Generator    (noise = input_shape_generator, features = ds_features, verbose = verbose)    \n",
        "\n",
        "    self.disc_optimizer = tf.optimizers.Adam(1e-4)\n",
        "    self.gen_optimizer  = tf.optimizers.Adam(1e-5)\n",
        "\n",
        "    self.input_shape_generator = input_shape_generator\n",
        "    self.verbose = verbose\n",
        "    pass\n",
        "\n",
        "  def call(self, N):\n",
        "    '''\n",
        "    Generate N records\n",
        "    '''\n",
        "\n",
        "    fake_data_noise = np.random.randn(N, self.input_shape_generator).astype(\"float32\")\n",
        "\n",
        "    generated_data = self.generator.model(fake_data_noise)\n",
        "\n",
        "    return generated_data\n",
        "  \n",
        "  def fit(self, dataset, epochs, verbose = False):\n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "      if verbose == True:\n",
        "        print(\"EPOCH: \" + str(epoch))\n",
        "      \n",
        "      data_index = 1\n",
        "      for data in dataset:\n",
        "\n",
        "        if verbose == True:\n",
        "          percentage = str(round(data_index * 100 / len(dataset),2)) + \"%\"\n",
        "          print(percentage, end=\":\\n\")\n",
        "\n",
        "        gen_loss, disc_loss = self.train_step(data, verbose)\n",
        "\n",
        "        data_index += 1\n",
        "\n",
        "    if self.verbose == True:\n",
        "      if (epoch + 1) % 50 == 0:\n",
        "        print()\n",
        "        print(\"EPOCH: \" + str(epoch + 1))\n",
        "        print(\"Generator loss: \" + str(gen_loss), end=\",\\t\")\n",
        "        print(\"Discriminator loss: \" + str(disc_loss))\n",
        "        \n",
        "      #output.clear()\n",
        "\n",
        "  def evaluate(self, classifier, test_size, cluster_value):\n",
        "    generated_data = self.call(test_size)\n",
        "\n",
        "    predictions = classifier.predict(generated_data)\n",
        "\n",
        "    predictions = [np.argmax(prediction) for prediction in predictions]\n",
        "    print(f\"Accuracy: {predictions.count(cluster_value) / len(predictions)}\")\n",
        "\n",
        "  def train_step(self, data, verbose):\n",
        "    fake_data_noise = np.random.randn(data[0].shape[0], self.input_shape_generator).astype(\"float32\")\n",
        "    \n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      \n",
        "      generated_data = self.generator.model(fake_data_noise)\n",
        "      real_data      = data[0]\n",
        "\n",
        "      if verbose == True:\n",
        "        print(\"1. Generated data\")\n",
        "\n",
        "      real_output = self.discriminator.model(real_data)\n",
        "      fake_output = self.discriminator.model(generated_data)\n",
        "      \n",
        "      if verbose == True:\n",
        "        print(real_output.shape)\n",
        "        print(fake_output.shape)\n",
        "\n",
        "        print(\"2. Got output from discriminator\")\n",
        "\n",
        "      gen_loss  = self.generator    .get_loss(fake_output)\n",
        "      \n",
        "      if verbose == True:\n",
        "        print(\"3.a Got loss from generator!\")\n",
        "\n",
        "      disc_loss = self.discriminator.get_loss(real_output, fake_output)\n",
        "\n",
        "      if verbose == True:\n",
        "        print(\"3.b Got loss from discriminator!\")\n",
        "\n",
        "        print(\"3. Got losses\")\n",
        "\n",
        "      generator_gradients     = gen_tape.gradient (gen_loss,  self.generator    .model.trainable_variables)\n",
        "      discriminator_gradients = disc_tape.gradient(disc_loss, self.discriminator.model.trainable_variables)\n",
        "\n",
        "      if verbose == True:\n",
        "        print(\"4. Loaded gradients\")\n",
        "\n",
        "      self.gen_optimizer .apply_gradients(zip(generator_gradients,     self.generator.model.trainable_variables))\n",
        "      self.disc_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator.model.trainable_variables))\n",
        "\n",
        "      if verbose == True:\n",
        "        print(\"5. Applied gradients\\n\")\n",
        "\n",
        "        print(\"Generator loss: \" + str(round(np.mean(gen_loss), 2)), end=\",\\t\")\n",
        "        print(\"Discriminator loss: \" + str(round(np.mean(disc_loss), 2)))\n",
        "\n",
        "      return np.mean(gen_loss), np.mean(disc_loss)"
      ],
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geMOLV_FG4yc"
      },
      "source": [
        "#### TGAN training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLcnrIFDRrrB"
      },
      "source": [
        "### Training and evaluation pipeline\n",
        "\n",
        "def train_and_evaluate_tgan(evaluation_classifier, test_size, epochs, batch_size = 32):\n",
        "  \n",
        "  for cluster_value in range(len(data.cluster.unique())):\n",
        "    '''\n",
        "    Load data of a specific cluster\n",
        "    '''\n",
        "    print(\"###\")\n",
        "    print(f\"> Cluster {cluster_value}\")\n",
        "    cluster = load_cluster(cluster_value)\n",
        "    X = cluster.drop(\"cluster\", axis = 1)\n",
        "    y = cluster.cluster - (cluster_value - 1) # Ones that correspond - for the classifier - to \"real data\"\n",
        "\n",
        "    '''\n",
        "    Build TensorFlow train and test Datasets\n",
        "    '''\n",
        "    train_ds = df_to_ds(X, y, batch_size = batch_size)\n",
        "\n",
        "    print(\"> Dataset built.\")\n",
        "\n",
        "    '''\n",
        "    Train and evaluate TGAN\n",
        "    '''\n",
        "    tgan = TGAN(ds_features = 131, input_shape_generator = 32, verbose = False)\n",
        "\n",
        "    print(\"> Start training.\")\n",
        "\n",
        "    tgan.fit(\n",
        "      train_ds,\n",
        "      epochs,\n",
        "      verbose = None\n",
        "    )\n",
        "\n",
        "    print(f\"> Trained TGAN for cluster {cluster_value} data.\")\n",
        "    \n",
        "    print(\"> Evaluation on pretrained multi-class classifier:\", end = \"\\t\")\n",
        "    tgan.evaluate(classifier = nn, test_size = 1000, cluster_value = cluster_value)\n",
        "    print(\"###\", end = \"\\n\\n\")"
      ],
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfFBI8GGU7Uo",
        "outputId": "94d10191-01a6-4d8a-cc19-8729791a1f0b"
      },
      "source": [
        "train_and_evaluate_tgan(evaluation_classifier = nn, test_size = 100, epochs = 100, batch_size = 32)"
      ],
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###\n",
            "> Cluster 0\n",
            "> Dataset built.\n",
            "> Start training.\n",
            "> Trained TGAN for cluster 0 data.\n",
            "> Evaluation on pretrained multi-class classifier:\tAccuracy: 1.0\n",
            "###\n",
            "\n",
            "###\n",
            "> Cluster 1\n",
            "> Dataset built.\n",
            "> Start training.\n",
            "> Trained TGAN for cluster 1 data.\n",
            "> Evaluation on pretrained multi-class classifier:\tAccuracy: 1.0\n",
            "###\n",
            "\n",
            "###\n",
            "> Cluster 2\n",
            "> Dataset built.\n",
            "> Start training.\n",
            "> Trained TGAN for cluster 2 data.\n",
            "> Evaluation on pretrained multi-class classifier:\tAccuracy: 0.999\n",
            "###\n",
            "\n",
            "###\n",
            "> Cluster 3\n",
            "> Dataset built.\n",
            "> Start training.\n",
            "> Trained TGAN for cluster 3 data.\n",
            "> Evaluation on pretrained multi-class classifier:\tAccuracy: 1.0\n",
            "###\n",
            "\n",
            "###\n",
            "> Cluster 4\n",
            "> Dataset built.\n",
            "> Start training.\n",
            "> Trained TGAN for cluster 4 data.\n",
            "> Evaluation on pretrained multi-class classifier:\tAccuracy: 0.989\n",
            "###\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRvMjwr0C-fh"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsmIYq2rC9Cv"
      },
      "source": [
        "def remodel_dataset(df, is_cluster = False):\n",
        "\n",
        "  if is_cluster == True:\n",
        "\n",
        "    indexes = df[\"subjects\"]\n",
        "    new_df  = pd.DataFrame(df[\"cluster.id\"].values, index=indexes, columns = [\"cluster\"])\n",
        "\n",
        "  else:\n",
        "\n",
        "    headers = df.iloc[0]\n",
        "    headers.name = \"subjects\"\n",
        "\n",
        "    indexes = df.index[1:]\n",
        "    new_df  = pd.DataFrame(df.values[1:], columns=headers, index=indexes)\n",
        "\n",
        "  return new_df\n",
        "\n",
        "def load_data(METH_FILENAME, CLUSTERS_FILENAME, MRNA_FILENAME, PROTEINS_FILENAME):\n",
        "  meth_data     = pd.read_csv(METH_FILENAME,     sep=\"\\t\").T\n",
        "  cluster_data  = pd.read_csv(CLUSTERS_FILENAME, sep=\"\\t\")\n",
        "  mRNA_data     = pd.read_csv(MRNA_FILENAME,     sep=\"\\t\").T\n",
        "  proteins_data = pd.read_csv(PROTEINS_FILENAME, sep=\"\\t\").T\n",
        "\n",
        "  meth_data     = remodel_dataset(meth_data)\n",
        "  cluster_data  = remodel_dataset(cluster_data, is_cluster = True)\n",
        "  mRNA_data     = remodel_dataset(mRNA_data)\n",
        "  proteins_data = remodel_dataset(proteins_data)\n",
        "  \n",
        "  return meth_data, cluster_data, mRNA_data, proteins_data\n",
        "\n",
        "def normalize_dataframe(data):\n",
        "  labels = data.pop(\"cluster\").values\n",
        "  new_data = pd.DataFrame(MinMaxScaler().fit_transform(data))\n",
        "  new_data[\"cluster\"] = labels\n",
        "  return new_data\n",
        "\n",
        "def load_cluster(cluster = None):\n",
        "  if cluster == None:\n",
        "    print(f\"No cluster passed! Choose one cluster between {sorted(data.cluster.unique())} :)\")\n",
        "    return\n",
        "\n",
        "  return data[data.cluster == cluster] \n",
        "\n",
        "def df_to_ds(dataframe, labels, shuffle=True, batch_size=32):\n",
        "  # A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "\n",
        "  dataframe = dataframe.copy()\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dataframe, labels))\n",
        "  \n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  \n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 365,
      "outputs": []
    }
  ]
}